{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9e9855-350d-4727-a74b-3dbeb22f301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bloque 1: Importaciones y docstring inicial\n",
    "\n",
    "\"\"\"\n",
    "Temporal Fusion Transformer Optimizado\n",
    "-----------------------------------------\n",
    "\n",
    "Esta versión implementa las optimizaciones generales propuestas en\n",
    "el código original publicado en 2020. Se ha estructurado el código para mejorar su\n",
    "legibilidad, eficiencia computacional y mantenibilidad, sin alterar la lógica esencial\n",
    "del modelo.\n",
    "\n",
    "Modificaciones destacadas:\n",
    "- Normalización de argumentos opcionales al inicio de __init__\n",
    "- Inicialización optimizada de estados LSTM usando unsqueeze/expand\n",
    "- Opción para compilar el método forward con torch.compile (PyTorch 2.0+)\n",
    "- Eliminación de código comentado redundante\n",
    "- Documentación detallada (docstrings) para cambios y funciones clave\n",
    "- NUEVAS OPTIMIZACIONES: Cacheo de máscaras, fusión de operaciones, \n",
    "  vectorización y funciones JIT para mejorar rendimiento\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "from copy import copy\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchmetrics import Metric as LightningMetric\n",
    "\n",
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "from pytorch_forecasting.metrics import (\n",
    "    MAE,\n",
    "    MAPE,\n",
    "    RMSE,\n",
    "    SMAPE,\n",
    "    MultiHorizonMetric,\n",
    "    QuantileLoss,\n",
    ")\n",
    "from pytorch_forecasting.models.base_model import BaseModelWithCovariates\n",
    "from pytorch_forecasting.models.nn import LSTM, MultiEmbedding\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.sub_modules import (\n",
    "    AddNorm,\n",
    "    GateAddNorm,\n",
    "    GatedLinearUnit,\n",
    "    GatedResidualNetwork,\n",
    "    InterpretableMultiHeadAttention,\n",
    "    VariableSelectionNetwork,\n",
    ")\n",
    "from pytorch_forecasting.utils import (\n",
    "    create_mask,\n",
    "    detach,\n",
    "    integer_histogram,\n",
    "    masked_op,\n",
    "    padded_stack,\n",
    "    to_list,\n",
    ")\n",
    "from pytorch_forecasting.utils._dependencies import _check_matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0873c74-dea9-44aa-a98d-6af9569046f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bloque 2: Definición de la clase principal y funciones auxiliares optimizadas\n",
    "\n",
    "class TemporalFusionTransformer(BaseModelWithCovariates):\n",
    "    # Inicialización optimizada de estados LSTM\n",
    "    @staticmethod\n",
    "    @torch.jit.script\n",
    "    def initialize_lstm_states(hidden, cell, layers: int):\n",
    "        \"\"\"\n",
    "        Inicializa los estados LSTM de manera optimizada con JIT.\n",
    "        \n",
    "        Args:\n",
    "            hidden: Estado hidden inicial.\n",
    "            cell: Estado cell inicial.\n",
    "            layers: Número de capas LSTM.\n",
    "            \n",
    "        Returns:\n",
    "            Tupla de estados inicializados (hidden, cell).\n",
    "        \"\"\"\n",
    "        return (\n",
    "            hidden.unsqueeze(0).expand(layers, -1, -1),\n",
    "            cell.unsqueeze(0).expand(layers, -1, -1)\n",
    "        )\n",
    "\n",
    "    # Fusión de operaciones en el procesamiento LSTM\n",
    "    def process_lstm_output(self, lstm_output, residual_input, is_encoder=True):\n",
    "        \"\"\"\n",
    "        Combina las operaciones de gate y add_norm para el procesamiento post-LSTM.\n",
    "        \n",
    "        Args:\n",
    "            lstm_output: Salida del LSTM.\n",
    "            residual_input: Entrada residual para la conexión skip.\n",
    "            is_encoder: Si se procesa el encoder (True) o decoder (False).\n",
    "            \n",
    "        Returns:\n",
    "            Salida procesada con gate y add_norm.\n",
    "        \"\"\"\n",
    "        gate = self.post_lstm_gate_encoder if is_encoder else self.post_lstm_gate_decoder\n",
    "        add_norm = self.post_lstm_add_norm_encoder if is_encoder else self.post_lstm_add_norm_decoder\n",
    "        return add_norm(gate(lstm_output), residual_input)\n",
    "\n",
    "    # Atención eficiente en memoria\n",
    "    def efficient_attention(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        Implementación de atención eficiente en memoria para secuencias largas.\n",
    "        \n",
    "        Args:\n",
    "            q: Queries.\n",
    "            k: Keys.\n",
    "            v: Values.\n",
    "            mask: Máscara para la atención.\n",
    "            \n",
    "        Returns:\n",
    "            Tupla de (salida de atención, pesos de atención).\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = q.shape\n",
    "        head_dim = d_model // self.hparams.attention_head_size\n",
    "        \n",
    "        # Reshape para computación por cabezas\n",
    "        q = q.view(batch_size, seq_len, self.hparams.attention_head_size, head_dim)\n",
    "        k = k.view(batch_size, -1, self.hparams.attention_head_size, head_dim)\n",
    "        v = v.view(batch_size, -1, self.hparams.attention_head_size, head_dim)\n",
    "        \n",
    "        # Calcular atención eficientemente usando operaciones por lotes\n",
    "        scores = torch.einsum(\"bqhd,bkhd->bhqk\", q, k) / math.sqrt(head_dim)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(~mask.unsqueeze(1), -1e9)\n",
    "            \n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        context = torch.einsum(\"bhqk,bkhd->bqhd\", attn_weights, v)\n",
    "        \n",
    "        return context.reshape(batch_size, seq_len, d_model), attn_weights\n",
    "\n",
    "    # Vectorización del procesamiento de interpretación\n",
    "    def process_variables_importance(self, variables, lengths):\n",
    "        \"\"\"\n",
    "        Vectoriza el procesamiento de importancia de variables.\n",
    "        \n",
    "        Args:\n",
    "            variables: Variables a procesar.\n",
    "            lengths: Longitudes efectivas.\n",
    "            \n",
    "        Returns:\n",
    "            Importancia de variables procesada.\n",
    "        \"\"\"\n",
    "        mask = create_mask(variables.size(1), lengths).unsqueeze(-1)\n",
    "        masked_vars = variables.masked_fill(mask, 0.0).sum(dim=1)\n",
    "        return masked_vars / lengths.clamp_min(1).unsqueeze(-1)\n",
    "\n",
    "    # Procesamiento paralelo para salida multiobjetivo\n",
    "    def transform_multi_output(self, output):\n",
    "        \"\"\"\n",
    "        Procesa todos los outputs en paralelo en lugar de secuencialmente.\n",
    "        \n",
    "        Args:\n",
    "            output: Salida de la red.\n",
    "            \n",
    "        Returns:\n",
    "            Lista de outputs procesados para cada target.\n",
    "        \"\"\"\n",
    "        if self.n_targets > 1:\n",
    "            # Procesar todos los outputs en paralelo en lugar de secuencialmente\n",
    "            stacked_out = torch.stack([ol.weight for ol in self.output_layer])\n",
    "            stacked_bias = torch.stack([ol.bias for ol in self.output_layer])\n",
    "            \n",
    "            # Reshape para permitir multiplicación matricial en batch\n",
    "            reshaped_out = output.unsqueeze(1)  # [batch, 1, hidden]\n",
    "            transformed = torch.bmm(\n",
    "                reshaped_out.expand(-1, self.n_targets, -1),  # [batch, n_targets, hidden]\n",
    "                stacked_out.transpose(1, 2)  # [n_targets, hidden, output_size]\n",
    "            )\n",
    "            transformed = transformed + stacked_bias.unsqueeze(0)\n",
    "            return [transformed[:, i] for i in range(self.n_targets)]\n",
    "        else:\n",
    "            return self.output_layer(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115c9a08-f184-40ae-89e4-3fd73cf888e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bloque 3: Método de inicialización (init)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int = 16,\n",
    "        lstm_layers: int = 1,\n",
    "        dropout: float = 0.1,\n",
    "        output_size: Union[int, List[int]] = 7,\n",
    "        loss: MultiHorizonMetric = None,\n",
    "        attention_head_size: int = 4,\n",
    "        max_encoder_length: int = 10,\n",
    "        static_categoricals: Optional[List[str]] = None,\n",
    "        static_reals: Optional[List[str]] = None,\n",
    "        time_varying_categoricals_encoder: Optional[List[str]] = None,\n",
    "        time_varying_categoricals_decoder: Optional[List[str]] = None,\n",
    "        categorical_groups: Optional[Union[Dict, List[str]]] = None,\n",
    "        time_varying_reals_encoder: Optional[List[str]] = None,\n",
    "        time_varying_reals_decoder: Optional[List[str]] = None,\n",
    "        x_reals: Optional[List[str]] = None,\n",
    "        x_categoricals: Optional[List[str]] = None,\n",
    "        hidden_continuous_size: int = 8,\n",
    "        hidden_continuous_sizes: Optional[Dict[str, int]] = None,\n",
    "        embedding_sizes: Optional[Dict[str, Tuple[int, int]]] = None,\n",
    "        embedding_paddings: Optional[List[str]] = None,\n",
    "        embedding_labels: Optional[Dict[str, np.ndarray]] = None,\n",
    "        learning_rate: float = 1e-3,\n",
    "        log_interval: Union[int, float] = -1,\n",
    "        log_val_interval: Union[int, float] = None,\n",
    "        log_gradient_flow: bool = False,\n",
    "        reduce_on_plateau_patience: int = 1000,\n",
    "        monotone_constaints: Optional[Dict[str, int]] = None,\n",
    "        share_single_variable_networks: bool = False,\n",
    "        causal_attention: bool = True,\n",
    "        logging_metrics: Optional[nn.ModuleList] = None,\n",
    "        use_compile: bool = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Temporal Fusion Transformer para series temporales.\n",
    "        \n",
    "        Se han aplicado optimizaciones en el manejo de argumentos opcionales\n",
    "        y se añade la posibilidad de compilar el método forward para mejorar\n",
    "        el rendimiento en PyTorch 2.0+.\n",
    "\n",
    "        Args:\n",
    "            hidden_size: Tamaño de la capa oculta.\n",
    "            lstm_layers: Número de capas LSTM.\n",
    "            dropout: Tasa de dropout.\n",
    "            output_size: Número de salidas (p.ej.: número de cuantiles en QuantileLoss).\n",
    "            loss: Función de pérdida (debe ser un LightningMetric).\n",
    "            attention_head_size: Número de cabezas en la atención.\n",
    "            max_encoder_length: Longitud máxima del encoder.\n",
    "            static_categoricals: Lista de variables categóricas estáticas.\n",
    "            static_reals: Lista de variables continuas estáticas.\n",
    "            time_varying_categoricals_encoder: Lista de variables categóricas para el encoder.\n",
    "            time_varying_categoricals_decoder: Lista de variables categóricas para el decoder.\n",
    "            categorical_groups: Diccionario o lista de grupos de variables categóricas.\n",
    "            time_varying_reals_encoder: Lista de variables continuas para el encoder.\n",
    "            time_varying_reals_decoder: Lista de variables continuas para el decoder.\n",
    "            x_reals: Orden de variables continuas en el tensor de entrada.\n",
    "            x_categoricals: Orden de variables categóricas en el tensor de entrada.\n",
    "            hidden_continuous_size: Tamaño oculto para variables continuas.\n",
    "            hidden_continuous_sizes: Diccionario que mapea variables continuas a tamaños específicos.\n",
    "            embedding_sizes: Diccionario que mapea nombres de variables categóricas a tuplas (número de clases, tamaño del embedding).\n",
    "            embedding_paddings: Lista de variables categóricas con padding.\n",
    "            embedding_labels: Diccionario que mapea nombres de variables categóricas a etiquetas.\n",
    "            learning_rate: Tasa de aprendizaje.\n",
    "            log_interval: Intervalo para logging de predicciones.\n",
    "            log_val_interval: Intervalo para logging en validación.\n",
    "            log_gradient_flow: Si se debe loggear el flujo de gradientes.\n",
    "            reduce_on_plateau_patience: Paciencia para reducir la tasa de aprendizaje.\n",
    "            monotone_constaints: Restricciones de monotonía para variables continuas.\n",
    "            share_single_variable_networks: Si compartir la red de variable única entre encoder y decoder.\n",
    "            causal_attention: Si se aplica atención causal en el decoder.\n",
    "            logging_metrics: Lista de métricas a loggear durante el entrenamiento.\n",
    "            use_compile: Si se debe compilar el método forward (requiere PyTorch 2.0+).\n",
    "            **kwargs: Argumentos adicionales para BaseModel.\n",
    "        \"\"\"\n",
    "        # Normalización de argumentos opcionales para evitar múltiples comprobaciones posteriores\n",
    "        static_categoricals = static_categoricals or []\n",
    "        static_reals = static_reals or []\n",
    "        time_varying_categoricals_encoder = time_varying_categoricals_encoder or []\n",
    "        time_varying_categoricals_decoder = time_varying_categoricals_decoder or []\n",
    "        time_varying_reals_encoder = time_varying_reals_encoder or []\n",
    "        time_varying_reals_decoder = time_varying_reals_decoder or []\n",
    "        x_categoricals = x_categoricals or []\n",
    "        x_reals = x_reals or []\n",
    "        embedding_labels = embedding_labels or {}\n",
    "        embedding_paddings = embedding_paddings or []\n",
    "        embedding_sizes = embedding_sizes or {}\n",
    "        hidden_continuous_sizes = hidden_continuous_sizes or {}\n",
    "        categorical_groups = categorical_groups or {}\n",
    "        if monotone_constaints is None:\n",
    "            monotone_constaints = {}\n",
    "        if logging_metrics is None:\n",
    "            logging_metrics = nn.ModuleList([SMAPE(), MAE(), RMSE(), MAPE()])\n",
    "        if loss is None:\n",
    "            loss = QuantileLoss()\n",
    "\n",
    "        # Se guardan los hiperparámetros (incluyendo los que ya tienen valor por defecto)\n",
    "        super().__init__(loss=loss, logging_metrics=logging_metrics, **kwargs)\n",
    "        self.save_hyperparameters(ignore=[\"use_compile\"])\n",
    "        self.hparams.use_compile = use_compile  # almacenar flag de compilación\n",
    "\n",
    "        # Creación de los módulos de procesamiento de inputs\n",
    "        # 1. Embeddings para variables categóricas\n",
    "        self.input_embeddings = MultiEmbedding(\n",
    "            embedding_sizes=self.hparams.embedding_sizes,\n",
    "            categorical_groups=self.hparams.categorical_groups,\n",
    "            embedding_paddings=self.hparams.embedding_paddings,\n",
    "            x_categoricals=self.hparams.x_categoricals,\n",
    "            max_embedding_size=self.hparams.hidden_size,\n",
    "        )\n",
    "\n",
    "        # 2. Procesamiento de variables continuas a través de capas lineales (prescalers)\n",
    "        self.prescalers = nn.ModuleDict(\n",
    "            {\n",
    "                name: nn.Linear(\n",
    "                    1,\n",
    "                    self.hparams.hidden_continuous_sizes.get(\n",
    "                        name, self.hparams.hidden_continuous_size\n",
    "                    ),\n",
    "                )\n",
    "                for name in self.reals  # se asume que self.reals se define en la clase base\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # 3. Variable Selection para variables estáticas, encoder y decoder\n",
    "        # Variables estáticas\n",
    "        static_input_sizes = {\n",
    "            name: self.input_embeddings.output_size[name]\n",
    "            for name in self.hparams.static_categoricals\n",
    "        }\n",
    "        static_input_sizes.update(\n",
    "            {\n",
    "                name: self.hparams.hidden_continuous_sizes.get(\n",
    "                    name, self.hparams.hidden_continuous_size\n",
    "                )\n",
    "                for name in self.hparams.static_reals\n",
    "            }\n",
    "        )\n",
    "        self.static_variable_selection = VariableSelectionNetwork(\n",
    "            input_sizes=static_input_sizes,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            input_embedding_flags={\n",
    "                name: True for name in self.hparams.static_categoricals\n",
    "            },\n",
    "            dropout=self.hparams.dropout,\n",
    "            prescalers=self.prescalers,\n",
    "        )\n",
    "\n",
    "        # Variables para encoder y decoder\n",
    "        encoder_input_sizes = {\n",
    "            name: self.input_embeddings.output_size[name]\n",
    "            for name in self.hparams.time_varying_categoricals_encoder\n",
    "        }\n",
    "        encoder_input_sizes.update(\n",
    "            {\n",
    "                name: self.hparams.hidden_continuous_sizes.get(\n",
    "                    name, self.hparams.hidden_continuous_size\n",
    "                )\n",
    "                for name in self.hparams.time_varying_reals_encoder\n",
    "            }\n",
    "        )\n",
    "        decoder_input_sizes = {\n",
    "            name: self.input_embeddings.output_size[name]\n",
    "            for name in self.hparams.time_varying_categoricals_decoder\n",
    "        }\n",
    "        decoder_input_sizes.update(\n",
    "            {\n",
    "                name: self.hparams.hidden_continuous_sizes.get(\n",
    "                    name, self.hparams.hidden_continuous_size\n",
    "                )\n",
    "                for name in self.hparams.time_varying_reals_decoder\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Opción de compartir redes de variable única entre encoder y decoder\n",
    "        if self.hparams.share_single_variable_networks:\n",
    "            self.shared_single_variable_grns = nn.ModuleDict()\n",
    "            for name, input_size in encoder_input_sizes.items():\n",
    "                self.shared_single_variable_grns[name] = GatedResidualNetwork(\n",
    "                    input_size,\n",
    "                    min(input_size, self.hparams.hidden_size),\n",
    "                    self.hparams.hidden_size,\n",
    "                    self.hparams.dropout,\n",
    "                )\n",
    "            for name, input_size in decoder_input_sizes.items():\n",
    "                if name not in self.shared_single_variable_grns:\n",
    "                    self.shared_single_variable_grns[name] = GatedResidualNetwork(\n",
    "                        input_size,\n",
    "                        min(input_size, self.hparams.hidden_size),\n",
    "                        self.hparams.hidden_size,\n",
    "                        self.hparams.dropout,\n",
    "                    )\n",
    "\n",
    "        self.encoder_variable_selection = VariableSelectionNetwork(\n",
    "            input_sizes=encoder_input_sizes,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            input_embedding_flags={\n",
    "                name: True for name in self.hparams.time_varying_categoricals_encoder\n",
    "            },\n",
    "            dropout=self.hparams.dropout,\n",
    "            context_size=self.hparams.hidden_size,\n",
    "            prescalers=self.prescalers,\n",
    "            single_variable_grns=(\n",
    "                {} if not self.hparams.share_single_variable_networks else self.shared_single_variable_grns\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.decoder_variable_selection = VariableSelectionNetwork(\n",
    "            input_sizes=decoder_input_sizes,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            input_embedding_flags={\n",
    "                name: True for name in self.hparams.time_varying_categoricals_decoder\n",
    "            },\n",
    "            dropout=self.hparams.dropout,\n",
    "            context_size=self.hparams.hidden_size,\n",
    "            prescalers=self.prescalers,\n",
    "            single_variable_grns=(\n",
    "                {} if not self.hparams.share_single_variable_networks else self.shared_single_variable_grns\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # 4. Codificadores estáticos para variable selection, estado inicial y enriquecimiento\n",
    "        self.static_context_variable_selection = GatedResidualNetwork(\n",
    "            input_size=self.hparams.hidden_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            output_size=self.hparams.hidden_size,\n",
    "            dropout=self.hparams.dropout,\n",
    "        )\n",
    "        self.static_context_initial_hidden_lstm = GatedResidualNetwork(\n",
    "            input_size=self.hparams.hidden_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            output_size=self.hparams.hidden_size,\n",
    "            dropout=self.hparams.dropout,\n",
    "        )\n",
    "        self.static_context_initial_cell_lstm = GatedResidualNetwork(\n",
    "            input_size=self.hparams.hidden_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            output_size=self.hparams.hidden_size,\n",
    "            dropout=self.hparams.dropout,\n",
    "        )\n",
    "        self.static_context_enrichment = GatedResidualNetwork(\n",
    "            self.hparams.hidden_size,\n",
    "            self.hparams.hidden_size,\n",
    "            self.hparams.hidden_size,\n",
    "            self.hparams.dropout,\n",
    "        )\n",
    "\n",
    "        # 5. LSTM para procesamiento local: encoder (histórico) y decoder (futuro)\n",
    "        self.lstm_encoder = LSTM(\n",
    "            input_size=self.hparams.hidden_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            num_layers=self.hparams.lstm_layers,\n",
    "            dropout=self.hparams.dropout if self.hparams.lstm_layers > 1 else 0,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.lstm_decoder = LSTM(\n",
    "            input_size=self.hparams.hidden_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            num_layers=self.hparams.lstm_layers,\n",
    "            dropout=self.hparams.dropout if self.hparams.lstm_layers > 1 else 0,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # 6. Skip connections y normalización post-LSTM\n",
    "        self.post_lstm_gate_encoder = GatedLinearUnit(\n",
    "            self.hparams.hidden_size, dropout=self.hparams.dropout\n",
    "        )\n",
    "        # Reutilizamos la misma instancia para decoder para simplificar\n",
    "        self.post_lstm_gate_decoder = self.post_lstm_gate_encoder\n",
    "        self.post_lstm_add_norm_encoder = AddNorm(\n",
    "            self.hparams.hidden_size, trainable_add=False\n",
    "        )\n",
    "        self.post_lstm_add_norm_decoder = self.post_lstm_add_norm_encoder\n",
    "\n",
    "        # 7. Enriquecimiento estático y atención para procesamiento a largo alcance\n",
    "        self.static_enrichment = GatedResidualNetwork(\n",
    "            input_size=self.hparams.hidden_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            output_size=self.hparams.hidden_size,\n",
    "            dropout=self.hparams.dropout,\n",
    "            context_size=self.hparams.hidden_size,\n",
    "        )\n",
    "        self.multihead_attn = InterpretableMultiHeadAttention(\n",
    "            d_model=self.hparams.hidden_size,\n",
    "            n_head=self.hparams.attention_head_size,\n",
    "            dropout=self.hparams.dropout,\n",
    "        )\n",
    "        self.post_attn_gate_norm = GateAddNorm(\n",
    "            self.hparams.hidden_size, dropout=self.hparams.dropout, trainable_add=False\n",
    "        )\n",
    "        self.pos_wise_ff = GatedResidualNetwork(\n",
    "            self.hparams.hidden_size,\n",
    "            self.hparams.hidden_size,\n",
    "            self.hparams.hidden_size,\n",
    "            dropout=self.hparams.dropout,\n",
    "        )\n",
    "\n",
    "        # 8. Procesamiento de salida (sin dropout en esta etapa)\n",
    "        self.pre_output_gate_norm = GateAddNorm(\n",
    "            self.hparams.hidden_size, dropout=None, trainable_add=False\n",
    "        )\n",
    "        if self.n_targets > 1:  # arquitectura multiobjetivo\n",
    "            self.output_layer = nn.ModuleList(\n",
    "                [\n",
    "                    nn.Linear(self.hparams.hidden_size, osize)\n",
    "                    for osize in (self.hparams.output_size if isinstance(self.hparams.output_size, list)\n",
    "                                  else [self.hparams.output_size])\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            self.output_layer = nn.Linear(\n",
    "                self.hparams.hidden_size, self.hparams.output_size\n",
    "            )\n",
    "\n",
    "        # Cache para máscaras de atención\n",
    "        self._attention_mask_cache = {}\n",
    "        \n",
    "        # Compilación mejorada para otras funciones críticas\n",
    "        if self.hparams.use_compile and hasattr(torch, \"compile\"):\n",
    "            self.forward = torch.compile(self.forward)\n",
    "            self.interpret_output = torch.compile(\n",
    "                self.interpret_output,\n",
    "                dynamic=True  # Para manejar tamaños variables\n",
    "            )\n",
    "            # Se añade un docstring indicando que el método forward ha sido compilado\n",
    "            self.forward.__doc__ = (self.forward.__doc__ or \"\") + \"\\n\\nOptimized with torch.compile.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c742878-cf40-4898-a130-6af26193ab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Bloque 4: Métodos de utilidad\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataset(\n",
    "        cls,\n",
    "        dataset: TimeSeriesDataSet,\n",
    "        allowed_encoder_known_variable_names: List[str] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Crea el modelo a partir de un dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset: Dataset de series temporales.\n",
    "            allowed_encoder_known_variable_names: Lista de variables conocidas permitidas en el encoder.\n",
    "            **kwargs: Argumentos adicionales (p.ej. hiperparámetros).\n",
    "\n",
    "        Returns:\n",
    "            Instancia de TemporalFusionTransformer.\n",
    "        \"\"\"\n",
    "        new_kwargs = copy(kwargs)\n",
    "        new_kwargs[\"max_encoder_length\"] = dataset.max_encoder_length\n",
    "        new_kwargs.update(\n",
    "            cls.deduce_default_output_parameters(dataset, kwargs, QuantileLoss())\n",
    "        )\n",
    "        return super().from_dataset(\n",
    "            dataset,\n",
    "            allowed_encoder_known_variable_names=allowed_encoder_known_variable_names,\n",
    "            **new_kwargs,\n",
    "        )\n",
    "\n",
    "    def expand_static_context(self, context: torch.Tensor, timesteps: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Expande el contexto estático a dimensión temporal.\n",
    "\n",
    "        Args:\n",
    "            context: Tensor con el contexto estático (batch, features).\n",
    "            timesteps: Número de timesteps a expandir.\n",
    "\n",
    "        Returns:\n",
    "            Tensor expandido de dimensiones (batch, timesteps, features).\n",
    "        \"\"\"\n",
    "        return context.unsqueeze(1).expand(-1, timesteps, -1)\n",
    "\n",
    "    # Cache de máscaras de atención\n",
    "    def get_attention_mask(\n",
    "        self, encoder_lengths: torch.LongTensor, decoder_lengths: torch.LongTensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Genera una máscara causal para la capa de self-atención con cache.\n",
    "\n",
    "        Args:\n",
    "            encoder_lengths: Longitudes efectivas del encoder en el batch.\n",
    "            decoder_lengths: Longitudes efectivas del decoder en el batch.\n",
    "\n",
    "        Returns:\n",
    "            Tensor máscara combinado para el encoder y decoder.\n",
    "        \"\"\"\n",
    "        # Usar una clave de cache basada en las dimensiones\n",
    "        cache_key = (encoder_lengths.max().item(), decoder_lengths.max().item())\n",
    "        if cache_key not in self._attention_mask_cache:\n",
    "            decoder_length = decoder_lengths.max()\n",
    "            if self.hparams.causal_attention:\n",
    "                attend_step = torch.arange(decoder_length, device=self.device)\n",
    "                predict_step = torch.arange(decoder_length, device=self.device).unsqueeze(1)\n",
    "                decoder_mask = (attend_step >= predict_step).unsqueeze(0).expand(encoder_lengths.size(0), -1, -1)\n",
    "            else:\n",
    "                decoder_mask = create_mask(decoder_length, decoder_lengths).unsqueeze(1).expand(-1, decoder_length, -1)\n",
    "            encoder_mask = create_mask(encoder_lengths.max(), encoder_lengths).unsqueeze(1).expand(-1, decoder_length, -1)\n",
    "            self._attention_mask_cache[cache_key] = torch.cat((encoder_mask, decoder_mask), dim=2)\n",
    "        return self._attention_mask_cache[cache_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3befcb-70ee-4171-b443-b86a157048a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, x: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Ejecuta la propagación hacia adelante del modelo.\n",
    "\n",
    "        Las dimensiones de entrada se esperan como: (n_samples x time x variables).\n",
    "\n",
    "        Args:\n",
    "            x: Diccionario con tensores de entrada, incluyendo:\n",
    "                - \"encoder_cat\", \"decoder_cat\"\n",
    "                - \"encoder_cont\", \"decoder_cont\"\n",
    "                - \"encoder_lengths\", \"decoder_lengths\"\n",
    "                - \"target_scale\", entre otros\n",
    "\n",
    "        Returns:\n",
    "            Diccionario con la predicción y datos auxiliares (e.g., pesos de atención).\n",
    "        \"\"\"\n",
    "        # MEJORA 8: Precalcular dimensiones frecuentes\n",
    "        batch_size = x[\"encoder_lengths\"].size(0)\n",
    "        encoder_lengths = x[\"encoder_lengths\"]\n",
    "        decoder_lengths = x[\"decoder_lengths\"]\n",
    "        max_encoder_length = int(encoder_lengths.max())\n",
    "        \n",
    "        # Concatenar inputs de categorías y continuos en la dimensión temporal\n",
    "        x_cat = torch.cat([x[\"encoder_cat\"], x[\"decoder_cat\"]], dim=1)\n",
    "        x_cont = torch.cat([x[\"encoder_cont\"], x[\"decoder_cont\"]], dim=1)\n",
    "        timesteps = x_cont.size(1)\n",
    "        \n",
    "        input_vectors = self.input_embeddings(x_cat)\n",
    "        # Agregar variables continuas a partir del orden definido en x_reals\n",
    "        input_vectors.update(\n",
    "            {\n",
    "                name: x_cont[..., idx].unsqueeze(-1)\n",
    "                for idx, name in enumerate(self.hparams.x_reals)\n",
    "                if name in self.reals\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Variable selection para variables estáticas\n",
    "        if len(self.static_variables) > 0:\n",
    "            static_embedding = {name: input_vectors[name][:, 0] for name in self.static_variables}\n",
    "            static_embedding, static_variable_selection = self.static_variable_selection(static_embedding)\n",
    "        else:\n",
    "            static_embedding = torch.zeros(\n",
    "                (batch_size, self.hparams.hidden_size),\n",
    "                dtype=self.dtype, device=self.device\n",
    "            )\n",
    "            static_variable_selection = torch.zeros(\n",
    "                (batch_size, 0), dtype=self.dtype, device=self.device\n",
    "            )\n",
    "\n",
    "        static_context_variable_selection = self.expand_static_context(\n",
    "            self.static_context_variable_selection(static_embedding), timesteps\n",
    "        )\n",
    "\n",
    "        # Variable selection para encoder y decoder (a partir de embeddings)\n",
    "        embeddings_varying_encoder = {\n",
    "            name: input_vectors[name][:, :max_encoder_length]\n",
    "            for name in self.encoder_variables\n",
    "        }\n",
    "        embeddings_varying_encoder, encoder_sparse_weights = self.encoder_variable_selection(\n",
    "            embeddings_varying_encoder,\n",
    "            static_context_variable_selection[:, :max_encoder_length],\n",
    "        )\n",
    "        embeddings_varying_decoder = {\n",
    "            name: input_vectors[name][:, max_encoder_length:]\n",
    "            for name in self.decoder_variables\n",
    "        }\n",
    "        embeddings_varying_decoder, decoder_sparse_weights = self.decoder_variable_selection(\n",
    "            embeddings_varying_decoder,\n",
    "            static_context_variable_selection[:, max_encoder_length:],\n",
    "        )\n",
    "\n",
    "        # MEJORA 3: Inicialización optimizada de estados LSTM\n",
    "        init_hidden = self.static_context_initial_hidden_lstm(static_embedding)\n",
    "        init_cell = self.static_context_initial_cell_lstm(static_embedding)\n",
    "        input_hidden = init_hidden.unsqueeze(0).expand(self.hparams.lstm_layers, -1, -1)\n",
    "        input_cell = init_cell.unsqueeze(0).expand(self.hparams.lstm_layers, -1, -1)\n",
    "\n",
    "        # Procesamiento LSTM para encoder y decoder\n",
    "        encoder_output, (hidden, cell) = self.lstm_encoder(\n",
    "            embeddings_varying_encoder,\n",
    "            (input_hidden, input_cell),\n",
    "            lengths=encoder_lengths,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "        decoder_output, _ = self.lstm_decoder(\n",
    "            embeddings_varying_decoder,\n",
    "            (hidden, cell),\n",
    "            lengths=decoder_lengths,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "\n",
    "        # MEJORA 2: Procesamiento fusionado post-LSTM para mejorar eficiencia\n",
    "        lstm_output_encoder = self.process_lstm_output(encoder_output, embeddings_varying_encoder, True)\n",
    "        lstm_output_decoder = self.process_lstm_output(decoder_output, embeddings_varying_decoder, False)\n",
    "        lstm_output = torch.cat([lstm_output_encoder, lstm_output_decoder], dim=1)\n",
    "\n",
    "        # Enriquecimiento estático\n",
    "        static_context_enrichment = self.static_context_enrichment(static_embedding)\n",
    "        attn_input = self.static_enrichment(\n",
    "            lstm_output,\n",
    "            self.expand_static_context(static_context_enrichment, timesteps),\n",
    "        )\n",
    "\n",
    "        # MEJORA 7: Usar atención eficiente en memoria para secuencias largas\n",
    "        if hasattr(self, \"efficient_attention\") and max_encoder_length > 100:\n",
    "            attn_output, attn_output_weights = self.efficient_attention(\n",
    "                q=attn_input[:, max_encoder_length:],\n",
    "                k=attn_input,\n",
    "                v=attn_input,\n",
    "                mask=self.get_attention_mask(encoder_lengths=encoder_lengths, decoder_lengths=decoder_lengths),\n",
    "            )\n",
    "        else:\n",
    "            # Atención multi-cabeza con máscara (caso estándar)\n",
    "            attn_output, attn_output_weights = self.multihead_attn(\n",
    "                q=attn_input[:, max_encoder_length:],\n",
    "                k=attn_input,\n",
    "                v=attn_input,\n",
    "                mask=self.get_attention_mask(encoder_lengths=encoder_lengths, decoder_lengths=decoder_lengths),\n",
    "            )\n",
    "\n",
    "        attn_output = self.post_attn_gate_norm(attn_output, attn_input[:, max_encoder_length:])\n",
    "        output = self.pos_wise_ff(attn_output)\n",
    "\n",
    "        # Skip connection final antes de salida\n",
    "        output = self.pre_output_gate_norm(output, lstm_output[:, max_encoder_length:])\n",
    "        \n",
    "        # MEJORA 5: Procesamiento paralelo para salida multiobjetivo\n",
    "        if self.n_targets > 1:\n",
    "            output = self.transform_multi_output(output)\n",
    "        else:\n",
    "            output = self.output_layer(output)\n",
    "\n",
    "        return self.to_network_output(\n",
    "            prediction=self.transform_output(output, target_scale=x[\"target_scale\"]),\n",
    "            encoder_attention=attn_output_weights[..., :max_encoder_length],\n",
    "            decoder_attention=attn_output_weights[..., max_encoder_length:],\n",
    "            static_variables=static_variable_selection,\n",
    "            encoder_variables=encoder_sparse_weights,\n",
    "            decoder_variables=decoder_sparse_weights,\n",
    "            decoder_lengths=decoder_lengths,\n",
    "            encoder_lengths=encoder_lengths,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c18f41-05d8-4d66-a8f6-32561f5bd45a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
