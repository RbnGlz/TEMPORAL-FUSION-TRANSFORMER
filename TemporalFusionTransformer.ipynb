{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9e9855-350d-4727-a74b-3dbeb22f301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bloque 1: Importaciones y docstring inicial\n",
    "\n",
    "\"\"\"\n",
    "Temporal Fusion Transformer Optimizado\n",
    "-----------------------------------------\n",
    "\n",
    "Esta versión implementa las optimizaciones generales propuestas en\n",
    "el código original publicado en 2020. Se ha estructurado el código para mejorar su\n",
    "legibilidad, eficiencia computacional y mantenibilidad, sin alterar la lógica esencial\n",
    "del modelo.\n",
    "\n",
    "Modificaciones destacadas:\n",
    "- Normalización de argumentos opcionales al inicio de __init__\n",
    "- Inicialización optimizada de estados LSTM usando unsqueeze/expand\n",
    "- Opción para compilar el método forward con torch.compile (PyTorch 2.0+)\n",
    "- Eliminación de código comentado redundante\n",
    "- Documentación detallada (docstrings) para cambios y funciones clave\n",
    "- NUEVAS OPTIMIZACIONES: Cacheo de máscaras, fusión de operaciones, \n",
    "  vectorización y funciones JIT para mejorar rendimiento\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "from copy import copy\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchmetrics import Metric as LightningMetric\n",
    "\n",
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "from pytorch_forecasting.metrics import (\n",
    "    MAE,\n",
    "    MAPE,\n",
    "    RMSE,\n",
    "    SMAPE,\n",
    "    MultiHorizonMetric,\n",
    "    QuantileLoss,\n",
    ")\n",
    "from pytorch_forecasting.models.base_model import BaseModelWithCovariates\n",
    "from pytorch_forecasting.models.nn import LSTM, MultiEmbedding\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.sub_modules import (\n",
    "    AddNorm,\n",
    "    GateAddNorm,\n",
    "    GatedLinearUnit,\n",
    "    GatedResidualNetwork,\n",
    "    InterpretableMultiHeadAttention,\n",
    "    VariableSelectionNetwork,\n",
    ")\n",
    "from pytorch_forecasting.utils import (\n",
    "    create_mask,\n",
    "    detach,\n",
    "    integer_histogram,\n",
    "    masked_op,\n",
    "    padded_stack,\n",
    "    to_list,\n",
    ")\n",
    "from pytorch_forecasting.utils._dependencies import _check_matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0873c74-dea9-44aa-a98d-6af9569046f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bloque 2: Definición de la clase principal y funciones auxiliares optimizadas\n",
    "\n",
    "class TemporalFusionTransformer(BaseModelWithCovariates):\n",
    "    # Inicialización optimizada de estados LSTM\n",
    "    @staticmethod\n",
    "    @torch.jit.script\n",
    "    def initialize_lstm_states(hidden, cell, layers: int):\n",
    "        \"\"\"\n",
    "        Inicializa los estados LSTM de manera optimizada con JIT.\n",
    "        \n",
    "        Args:\n",
    "            hidden: Estado hidden inicial.\n",
    "            cell: Estado cell inicial.\n",
    "            layers: Número de capas LSTM.\n",
    "            \n",
    "        Returns:\n",
    "            Tupla de estados inicializados (hidden, cell).\n",
    "        \"\"\"\n",
    "        return (\n",
    "            hidden.unsqueeze(0).expand(layers, -1, -1),\n",
    "            cell.unsqueeze(0).expand(layers, -1, -1)\n",
    "        )\n",
    "\n",
    "    # Fusión de operaciones en el procesamiento LSTM\n",
    "    def process_lstm_output(self, lstm_output, residual_input, is_encoder=True):\n",
    "        \"\"\"\n",
    "        Combina las operaciones de gate y add_norm para el procesamiento post-LSTM.\n",
    "        \n",
    "        Args:\n",
    "            lstm_output: Salida del LSTM.\n",
    "            residual_input: Entrada residual para la conexión skip.\n",
    "            is_encoder: Si se procesa el encoder (True) o decoder (False).\n",
    "            \n",
    "        Returns:\n",
    "            Salida procesada con gate y add_norm.\n",
    "        \"\"\"\n",
    "        gate = self.post_lstm_gate_encoder if is_encoder else self.post_lstm_gate_decoder\n",
    "        add_norm = self.post_lstm_add_norm_encoder if is_encoder else self.post_lstm_add_norm_decoder\n",
    "        return add_norm(gate(lstm_output), residual_input)\n",
    "\n",
    "    # Atención eficiente en memoria\n",
    "    def efficient_attention(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        Implementación de atención eficiente en memoria para secuencias largas.\n",
    "        \n",
    "        Args:\n",
    "            q: Queries.\n",
    "            k: Keys.\n",
    "            v: Values.\n",
    "            mask: Máscara para la atención.\n",
    "            \n",
    "        Returns:\n",
    "            Tupla de (salida de atención, pesos de atención).\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = q.shape\n",
    "        head_dim = d_model // self.hparams.attention_head_size\n",
    "        \n",
    "        # Reshape para computación por cabezas\n",
    "        q = q.view(batch_size, seq_len, self.hparams.attention_head_size, head_dim)\n",
    "        k = k.view(batch_size, -1, self.hparams.attention_head_size, head_dim)\n",
    "        v = v.view(batch_size, -1, self.hparams.attention_head_size, head_dim)\n",
    "        \n",
    "        # Calcular atención eficientemente usando operaciones por lotes\n",
    "        scores = torch.einsum(\"bqhd,bkhd->bhqk\", q, k) / math.sqrt(head_dim)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(~mask.unsqueeze(1), -1e9)\n",
    "            \n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        context = torch.einsum(\"bhqk,bkhd->bqhd\", attn_weights, v)\n",
    "        \n",
    "        return context.reshape(batch_size, seq_len, d_model), attn_weights\n",
    "\n",
    "    # Vectorización del procesamiento de interpretación\n",
    "    def process_variables_importance(self, variables, lengths):\n",
    "        \"\"\"\n",
    "        Vectoriza el procesamiento de importancia de variables.\n",
    "        \n",
    "        Args:\n",
    "            variables: Variables a procesar.\n",
    "            lengths: Longitudes efectivas.\n",
    "            \n",
    "        Returns:\n",
    "            Importancia de variables procesada.\n",
    "        \"\"\"\n",
    "        mask = create_mask(variables.size(1), lengths).unsqueeze(-1)\n",
    "        masked_vars = variables.masked_fill(mask, 0.0).sum(dim=1)\n",
    "        return masked_vars / lengths.clamp_min(1).unsqueeze(-1)\n",
    "\n",
    "    # Procesamiento paralelo para salida multiobjetivo\n",
    "    def transform_multi_output(self, output):\n",
    "        \"\"\"\n",
    "        Procesa todos los outputs en paralelo en lugar de secuencialmente.\n",
    "        \n",
    "        Args:\n",
    "            output: Salida de la red.\n",
    "            \n",
    "        Returns:\n",
    "            Lista de outputs procesados para cada target.\n",
    "        \"\"\"\n",
    "        if self.n_targets > 1:\n",
    "            # Procesar todos los outputs en paralelo en lugar de secuencialmente\n",
    "            stacked_out = torch.stack([ol.weight for ol in self.output_layer])\n",
    "            stacked_bias = torch.stack([ol.bias for ol in self.output_layer])\n",
    "            \n",
    "            # Reshape para permitir multiplicación matricial en batch\n",
    "            reshaped_out = output.unsqueeze(1)  # [batch, 1, hidden]\n",
    "            transformed = torch.bmm(\n",
    "                reshaped_out.expand(-1, self.n_targets, -1),  # [batch, n_targets, hidden]\n",
    "                stacked_out.transpose(1, 2)  # [n_targets, hidden, output_size]\n",
    "            )\n",
    "            transformed = transformed + stacked_bias.unsqueeze(0)\n",
    "            return [transformed[:, i] for i in range(self.n_targets)]\n",
    "        else:\n",
    "            return self.output_layer(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115c9a08-f184-40ae-89e4-3fd73cf888e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bloque 3: Método de inicialización (init)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int = 16,\n",
    "        lstm_layers: int = 1,\n",
    "        dropout: float = 0.1,\n",
    "        output_size: Union[int, List[int]] = 7,\n",
    "        loss: MultiHorizonMetric = None,\n",
    "        attention_head_size: int = 4,\n",
    "        max_encoder_length: int = 10,\n",
    "        static_categoricals: Optional[List[str]] = None,\n",
    "        static_reals: Optional[List[str]] = None,\n",
    "        time_varying_categoricals_encoder: Optional[List[str]] = None,\n",
    "        time_varying_categoricals_decoder: Optional[List[str]] = None,\n",
    "        categorical_groups: Optional[Union[Dict, List[str]]] = None,\n",
    "        time_varying_reals_encoder: Optional[List[str]] = None,\n",
    "        time_varying_reals_decoder: Optional[List[str]] = None,\n",
    "        x_reals: Optional[List[str]] = None,\n",
    "        x_categoricals: Optional[List[str]] = None,\n",
    "        hidden_continuous_size: int = 8,\n",
    "        hidden_continuous_sizes: Optional[Dict[str, int]] = None,\n",
    "        embedding_sizes: Optional[Dict[str, Tuple[int, int]]] = None,\n",
    "        embedding_paddings: Optional[List[str]] = None,\n",
    "        embedding_labels: Optional[Dict[str, np.ndarray]] = None,\n",
    "        learning_rate: float = 1e-3,\n",
    "        log_interval: Union[int, float] = -1,\n",
    "        log_val_interval: Union[int, float] = None,\n",
    "        log_gradient_flow: bool = False,\n",
    "        reduce_on_plateau_patience: int = 1000,\n",
    "        monotone_constaints: Optional[Dict[str, int]] = None,\n",
    "        share_single_variable_networks: bool = False,\n",
    "        causal_attention: bool = True,\n",
    "        logging_metrics: Optional[nn.ModuleList] = None,\n",
    "        use_compile: bool = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Temporal Fusion Transformer para series temporales.\n",
    "        \n",
    "        Se han aplicado optimizaciones en el manejo de argumentos opcionales\n",
    "        y se añade la posibilidad de compilar el método forward para mejorar\n",
    "        el rendimiento en PyTorch 2.0+.\n",
    "\n",
    "        Args:\n",
    "            hidden_size: Tamaño de la capa oculta.\n",
    "            lstm_layers: Número de capas LSTM.\n",
    "            dropout: Tasa de dropout.\n",
    "            output_size: Número de salidas (p.ej.: número de cuantiles en QuantileLoss).\n",
    "            loss: Función de pérdida (debe ser un LightningMetric).\n",
    "            attention_head_size: Número de cabezas en la atención.\n",
    "            max_encoder_length: Longitud máxima del encoder.\n",
    "            static_categoricals: Lista de variables categóricas estáticas.\n",
    "            static_reals: Lista de variables continuas estáticas.\n",
    "            time_varying_categoricals_encoder: Lista de variables categóricas para el encoder.\n",
    "            time_varying_categoricals_decoder: Lista de variables categóricas para el decoder.\n",
    "            categorical_groups: Diccionario o lista de grupos de variables categóricas.\n",
    "            time_varying_reals_encoder: Lista de variables continuas para el encoder.\n",
    "            time_varying_reals_decoder: Lista de variables continuas para el decoder.\n",
    "            x_reals: Orden de variables continuas en el tensor de entrada.\n",
    "            x_categoricals: Orden de variables categóricas en el tensor de entrada.\n",
    "            hidden_continuous_size: Tamaño oculto para variables continuas.\n",
    "            hidden_continuous_sizes: Diccionario que mapea variables continuas a tamaños específicos.\n",
    "            embedding_sizes: Diccionario que mapea nombres de variables categóricas a tuplas (número de clases, tamaño del embedding).\n",
    "            embedding_paddings: Lista de variables categóricas con padding.\n",
    "            embedding_labels: Diccionario que mapea nombres de variables categóricas a etiquetas.\n",
    "            learning_rate: Tasa de aprendizaje.\n",
    "            log_interval: Intervalo para logging de predicciones.\n",
    "            log_val_interval: Intervalo para logging en validación.\n",
    "            log_gradient_flow: Si se debe loggear el flujo de gradientes.\n",
    "            reduce_on_plateau_patience: Paciencia para reducir la tasa de aprendizaje.\n",
    "            monotone_constaints: Restricciones de monotonía para variables continuas.\n",
    "            share_single_variable_networks: Si compartir la red de variable única entre encoder y decoder.\n",
    "            causal_attention: Si se aplica atención causal en el decoder.\n",
    "            logging_metrics: Lista de métricas a loggear durante el entrenamiento.\n",
    "            use_compile: Si se debe compilar el método forward (requiere PyTorch 2.0+).\n",
    "            **kwargs: Argumentos adicionales para BaseModel.\n",
    "        \"\"\"\n",
    "        # Normalización de argumentos opcionales para evitar múltiples comprobaciones posteriores\n",
    "        static_categoricals = static_categoricals or []\n",
    "        static_reals = static_reals or []\n",
    "        time_varying_categoricals_encoder = time_varying_categoricals_encoder or []\n",
    "        time_varying_categoricals_decoder = time_varying_categoricals_decoder or []\n",
    "        time_varying_reals_encoder = time_varying_reals_encoder or []\n",
    "        time_varying_reals_decoder = time_varying_reals_decoder or []\n",
    "        x_categoricals = x_categoricals or []\n",
    "        x_reals = x_reals or []\n",
    "        embedding_labels = embedding_labels or {}\n",
    "        embedding_paddings = embedding_paddings or []\n",
    "        embedding_sizes = embedding_sizes or {}\n",
    "        hidden_continuous_sizes = hidden_continuous_sizes or {}\n",
    "        categorical_groups = categorical_groups or {}\n",
    "        if monotone_constaints is None:\n",
    "            monotone_constaints = {}\n",
    "        if logging_metrics is None:\n",
    "            logging_metrics = nn.ModuleList([SMAPE(), MAE(), RMSE(), MAPE()])\n",
    "        if loss is None:\n",
    "            loss = QuantileLoss()\n",
    "\n",
    "        # Se guardan los hiperparámetros (incluyendo los que ya tienen valor por defecto)\n",
    "        super().__init__(loss=loss, logging_metrics=logging_metrics, **kwargs)\n",
    "        self.save_hyperparameters(ignore=[\"use_compile\"])\n",
    "        self.hparams.use_compile = use_compile  # almacenar flag de compilación\n",
    "\n",
    "        # Creación de los módulos de procesamiento de inputs\n",
    "        # 1. Embeddings para variables categóricas\n",
    "        self.input_embeddings = MultiEmbedding(\n",
    "            embedding_sizes=self.hparams.embedding_sizes,\n",
    "            categorical_groups=self.hparams.categorical_groups,\n",
    "            embedding_paddings=self.hparams.embedding_paddings,\n",
    "            x_categoricals=self.hparams.x_categoricals,\n",
    "            max_embedding_size=self.hparams.hidden_size,\n",
    "        )\n",
    "\n",
    "        # 2. Procesamiento de variables continuas a través de capas lineales (prescalers)\n",
    "        self.prescalers = nn.ModuleDict(\n",
    "            {\n",
    "                name: nn.Linear(\n",
    "                    1,\n",
    "                    self.hparams.hidden_continuous_sizes.get(\n",
    "                        name, self.hparams.hidden_continuous_size\n",
    "                    ),\n",
    "                )\n",
    "                for name in self.reals  # se asume que self.reals se define en la clase base\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # 3. Variable Selection para variables estáticas, encoder y decoder\n",
    "        # Variables estáticas\n",
    "        static_input_sizes = {\n",
    "            name: self.input_embeddings.output_size[name]\n",
    "            for name in self.hparams.static_categoricals\n",
    "        }\n",
    "        static_input_sizes.update(\n",
    "            {\n",
    "                name: self.hparams.hidden_continuous_sizes.get(\n",
    "                    name, self.hparams.hidden_continuous_size\n",
    "                )\n",
    "                for name in self.hparams.static_reals\n",
    "            }\n",
    "        )\n",
    "        self.static_variable_selection = VariableSelectionNetwork(\n",
    "            input_sizes=static_input_sizes,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            input_embedding_flags={\n",
    "                name: True for name in self.hparams.static_categoricals\n",
    "            },\n",
    "            dropout=self.hparams.dropout,\n",
    "            prescalers=self.prescalers,\n",
    "        )\n",
    "\n",
    "        # Variables para encoder y decoder\n",
    "        encoder_input_sizes = {\n",
    "            name: self.input_embeddings.output_size[name]\n",
    "            for name in self.hparams.time_varying_categoricals_encoder\n",
    "        }\n",
    "        encoder_input_sizes.update(\n",
    "            {\n",
    "                name: self.hparams.hidden_continuous_sizes.get(\n",
    "                    name, self.hparams.hidden_continuous_size\n",
    "                )\n",
    "                for name in self.hparams.time_varying_reals_encoder\n",
    "            }\n",
    "        )\n",
    "        decoder_input_sizes = {\n",
    "            name: self.input_embeddings.output_size[name]\n",
    "            for name in self.hparams.time_varying_categoricals_decoder\n",
    "        }\n",
    "        decoder_input_sizes.update(\n",
    "            {\n",
    "                name: self.hparams.hidden_continuous_sizes.get(\n",
    "                    name, self.hparams.hidden_continuous_size\n",
    "                )\n",
    "                for name in self.hparams.time_varying_reals_decoder\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Opción de compartir redes de variable única entre encoder y decoder\n",
    "        if self.hparams.share_single_variable_networks:\n",
    "            self.shared_single_variable_grns = nn.ModuleDict()\n",
    "            for name, input_size in encoder_input_sizes.items():\n",
    "                self.shared_single_variable_grns[name] = GatedResidualNetwork(\n",
    "                    input_size,\n",
    "                    min(input_size, self.hparams.hidden_size),\n",
    "                    self.hparams.hidden_size,\n",
    "                    self.hparams.dropout,\n",
    "                )\n",
    "            for name, input_size in decoder_input_sizes.items():\n",
    "                if name not in self.shared_single_variable_grns:\n",
    "                    self.shared_single_variable_grns[name] = GatedResidualNetwork(\n",
    "                        input_size,\n",
    "                        min(input_size, self.hparams.hidden_size),\n",
    "                        self.hparams.hidden_size,\n",
    "                        self.hparams.dropout,\n",
    "                    )\n",
    "\n",
    "        self.encoder_variable_selection = VariableSelectionNetwork(\n",
    "            input_sizes=encoder_input_sizes,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            input_embedding_flags={\n",
    "                name: True for name in self.hparams.time_varying_categoricals_encoder\n",
    "            },\n",
    "            dropout=self.hparams.dropout,\n",
    "            context_size=self.hparams.hidden_size,\n",
    "            prescalers=self.prescalers,\n",
    "            single_variable_grns=(\n",
    "                {} if not self.hparams.share_single_variable_networks else self.shared_single_variable_grns\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.decoder_variable_selection = VariableSelectionNetwork(\n",
    "            input_sizes=decoder_input_sizes,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            input_embedding_flags={\n",
    "                name: True for name in self.hparams.time_varying_categoricals_decoder\n",
    "            },\n",
    "            dropout=self.hparams.dropout,\n",
    "            context_size=self.hparams.hidden_size,\n",
    "            prescalers=self.prescalers,\n",
    "            single_variable_grns=(\n",
    "                {} if not self.hparams.share_single_variable_networks else self.shared_single_variable_grns\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # 4. Codificadores estáticos para variable selection, estado inicial y enriquecimiento\n",
    "        self.static_context_variable_selection = GatedResidualNetwork(\n",
    "            input_size=self.hparams.hidden_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            output_size=self.hparams.hidden_size,\n",
    "            dropout=self.hparams.dropout,\n",
    "        )\n",
    "        self.static_context_initial_hidden_lstm = GatedResidualNetwork(\n",
    "            input_size=self.hparams.hidden_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            output_size=self.hparams.hidden_size,\n",
    "            dropout=self.hparams.dropout,\n",
    "        )\n",
    "        self.static_context_initial_cell_lstm = GatedResidualNetwork(\n",
    "            input_size=self.hparams.hidden_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            output_size=self.hparams.hidden_size,\n",
    "            dropout=self.hparams.dropout,\n",
    "        )\n",
    "        self.static_context_enrichment = GatedResidualNetwork(\n",
    "            self.hparams.hidden_size,\n",
    "            self.hparams.hidden_size,\n",
    "            self.hparams.hidden_size,\n",
    "            self.hparams.dropout,\n",
    "        )\n",
    "\n",
    "        # 5. LSTM para procesamiento local: encoder (histórico) y decoder (futuro)\n",
    "        self.lstm_encoder = LSTM(\n",
    "            input_size=self.hparams.hidden_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            num_layers=self.hparams.lstm_layers,\n",
    "            dropout=self.hparams.dropout if self.hparams.lstm_layers > 1 else 0,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.lstm_decoder = LSTM(\n",
    "            input_size=self.hparams.hidden_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            num_layers=self.hparams.lstm_layers,\n",
    "            dropout=self.hparams.dropout if self.hparams.lstm_layers > 1 else 0,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # 6. Skip connections y normalización post-LSTM\n",
    "        self.post_lstm_gate_encoder = GatedLinearUnit(\n",
    "            self.hparams.hidden_size, dropout=self.hparams.dropout\n",
    "        )\n",
    "        # Reutilizamos la misma instancia para decoder para simplificar\n",
    "        self.post_lstm_gate_decoder = self.post_lstm_gate_encoder\n",
    "        self.post_lstm_add_norm_encoder = AddNorm(\n",
    "            self.hparams.hidden_size, trainable_add=False\n",
    "        )\n",
    "        self.post_lstm_add_norm_decoder = self.post_lstm_add_norm_encoder\n",
    "\n",
    "        # 7. Enriquecimiento estático y atención para procesamiento a largo alcance\n",
    "        self.static_enrichment = GatedResidualNetwork(\n",
    "            input_size=self.hparams.hidden_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            output_size=self.hparams.hidden_size,\n",
    "            dropout=self.hparams.dropout,\n",
    "            context_size=self.hparams.hidden_size,\n",
    "        )\n",
    "        self.multihead_attn = InterpretableMultiHeadAttention(\n",
    "            d_model=self.hparams.hidden_size,\n",
    "            n_head=self.hparams.attention_head_size,\n",
    "            dropout=self.hparams.dropout,\n",
    "        )\n",
    "        self.post_attn_gate_norm = GateAddNorm(\n",
    "            self.hparams.hidden_size, dropout=self.hparams.dropout, trainable_add=False\n",
    "        )\n",
    "        self.pos_wise_ff = GatedResidualNetwork(\n",
    "            self.hparams.hidden_size,\n",
    "            self.hparams.hidden_size,\n",
    "            self.hparams.hidden_size,\n",
    "            dropout=self.hparams.dropout,\n",
    "        )\n",
    "\n",
    "        # 8. Procesamiento de salida (sin dropout en esta etapa)\n",
    "        self.pre_output_gate_norm = GateAddNorm(\n",
    "            self.hparams.hidden_size, dropout=None, trainable_add=False\n",
    "        )\n",
    "        if self.n_targets > 1:  # arquitectura multiobjetivo\n",
    "            self.output_layer = nn.ModuleList(\n",
    "                [\n",
    "                    nn.Linear(self.hparams.hidden_size, osize)\n",
    "                    for osize in (self.hparams.output_size if isinstance(self.hparams.output_size, list)\n",
    "                                  else [self.hparams.output_size])\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            self.output_layer = nn.Linear(\n",
    "                self.hparams.hidden_size, self.hparams.output_size\n",
    "            )\n",
    "\n",
    "        # Cache para máscaras de atención\n",
    "        self._attention_mask_cache = {}\n",
    "        \n",
    "        # Compilación mejorada para otras funciones críticas\n",
    "        if self.hparams.use_compile and hasattr(torch, \"compile\"):\n",
    "            self.forward = torch.compile(self.forward)\n",
    "            self.interpret_output = torch.compile(\n",
    "                self.interpret_output,\n",
    "                dynamic=True  # Para manejar tamaños variables\n",
    "            )\n",
    "            # Se añade un docstring indicando que el método forward ha sido compilado\n",
    "            self.forward.__doc__ = (self.forward.__doc__ or \"\") + \"\\n\\nOptimized with torch.compile.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c742878-cf40-4898-a130-6af26193ab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Bloque 4: Métodos de utilidad\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataset(\n",
    "        cls,\n",
    "        dataset: TimeSeriesDataSet,\n",
    "        allowed_encoder_known_variable_names: List[str] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Crea el modelo a partir de un dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset: Dataset de series temporales.\n",
    "            allowed_encoder_known_variable_names: Lista de variables conocidas permitidas en el encoder.\n",
    "            **kwargs: Argumentos adicionales (p.ej. hiperparámetros).\n",
    "\n",
    "        Returns:\n",
    "            Instancia de TemporalFusionTransformer.\n",
    "        \"\"\"\n",
    "        new_kwargs = copy(kwargs)\n",
    "        new_kwargs[\"max_encoder_length\"] = dataset.max_encoder_length\n",
    "        new_kwargs.update(\n",
    "            cls.deduce_default_output_parameters(dataset, kwargs, QuantileLoss())\n",
    "        )\n",
    "        return super().from_dataset(\n",
    "            dataset,\n",
    "            allowed_encoder_known_variable_names=allowed_encoder_known_variable_names,\n",
    "            **new_kwargs,\n",
    "        )\n",
    "\n",
    "    def expand_static_context(self, context: torch.Tensor, timesteps: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Expande el contexto estático a dimensión temporal.\n",
    "\n",
    "        Args:\n",
    "            context: Tensor con el contexto estático (batch, features).\n",
    "            timesteps: Número de timesteps a expandir.\n",
    "\n",
    "        Returns:\n",
    "            Tensor expandido de dimensiones (batch, timesteps, features).\n",
    "        \"\"\"\n",
    "        return context.unsqueeze(1).expand(-1, timesteps, -1)\n",
    "\n",
    "    # Cache de máscaras de atención\n",
    "    def get_attention_mask(\n",
    "        self, encoder_lengths: torch.LongTensor, decoder_lengths: torch.LongTensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Genera una máscara causal para la capa de self-atención con cache.\n",
    "\n",
    "        Args:\n",
    "            encoder_lengths: Longitudes efectivas del encoder en el batch.\n",
    "            decoder_lengths: Longitudes efectivas del decoder en el batch.\n",
    "\n",
    "        Returns:\n",
    "            Tensor máscara combinado para el encoder y decoder.\n",
    "        \"\"\"\n",
    "        # Usar una clave de cache basada en las dimensiones\n",
    "        cache_key = (encoder_lengths.max().item(), decoder_lengths.max().item())\n",
    "        if cache_key not in self._attention_mask_cache:\n",
    "            decoder_length = decoder_lengths.max()\n",
    "            if self.hparams.causal_attention:\n",
    "                attend_step = torch.arange(decoder_length, device=self.device)\n",
    "                predict_step = torch.arange(decoder_length, device=self.device).unsqueeze(1)\n",
    "                decoder_mask = (attend_step >= predict_step).unsqueeze(0).expand(encoder_lengths.size(0), -1, -1)\n",
    "            else:\n",
    "                decoder_mask = create_mask(decoder_length, decoder_lengths).unsqueeze(1).expand(-1, decoder_length, -1)\n",
    "            encoder_mask = create_mask(encoder_lengths.max(), encoder_lengths).unsqueeze(1).expand(-1, decoder_length, -1)\n",
    "            self._attention_mask_cache[cache_key] = torch.cat((encoder_mask, decoder_mask), dim=2)\n",
    "        return self._attention_mask_cache[cache_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3befcb-70ee-4171-b443-b86a157048a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Bloque 5: Método forward\n",
    "\n",
    "    def forward(self, x: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Ejecuta la propagación hacia adelante del modelo.\n",
    "\n",
    "        Las dimensiones de entrada se esperan como: (n_samples x time x variables).\n",
    "\n",
    "        Args:\n",
    "            x: Diccionario con tensores de entrada, incluyendo:\n",
    "                - \"encoder_cat\", \"decoder_cat\"\n",
    "                - \"encoder_cont\", \"decoder_cont\"\n",
    "                - \"encoder_lengths\", \"decoder_lengths\"\n",
    "                - \"target_scale\", entre otros\n",
    "\n",
    "        Returns:\n",
    "            Diccionario con la predicción y datos auxiliares (e.g., pesos de atención).\n",
    "        \"\"\"\n",
    "        # Precalcular dimensiones frecuentes\n",
    "        batch_size = x[\"encoder_lengths\"].size(0)\n",
    "        encoder_lengths = x[\"encoder_lengths\"]\n",
    "        decoder_lengths = x[\"decoder_lengths\"]\n",
    "        max_encoder_length = int(encoder_lengths.max())\n",
    "        \n",
    "        # Concatenar inputs de categorías y continuos en la dimensión temporal\n",
    "        x_cat = torch.cat([x[\"encoder_cat\"], x[\"decoder_cat\"]], dim=1)\n",
    "        x_cont = torch.cat([x[\"encoder_cont\"], x[\"decoder_cont\"]], dim=1)\n",
    "        timesteps = x_cont.size(1)\n",
    "        \n",
    "        input_vectors = self.input_embeddings(x_cat)\n",
    "        # Agregar variables continuas a partir del orden definido en x_reals\n",
    "        input_vectors.update(\n",
    "            {\n",
    "                name: x_cont[..., idx].unsqueeze(-1)\n",
    "                for idx, name in enumerate(self.hparams.x_reals)\n",
    "                if name in self.reals\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Variable selection para variables estáticas\n",
    "        if len(self.static_variables) > 0:\n",
    "            static_embedding = {name: input_vectors[name][:, 0] for name in self.static_variables}\n",
    "            static_embedding, static_variable_selection = self.static_variable_selection(static_embedding)\n",
    "        else:\n",
    "            static_embedding = torch.zeros(\n",
    "                (batch_size, self.hparams.hidden_size),\n",
    "                dtype=self.dtype, device=self.device\n",
    "            )\n",
    "            static_variable_selection = torch.zeros(\n",
    "                (batch_size, 0), dtype=self.dtype, device=self.device\n",
    "            )\n",
    "\n",
    "        static_context_variable_selection = self.expand_static_context(\n",
    "            self.static_context_variable_selection(static_embedding), timesteps\n",
    "        )\n",
    "\n",
    "        # Variable selection para encoder y decoder (a partir de embeddings)\n",
    "        embeddings_varying_encoder = {\n",
    "            name: input_vectors[name][:, :max_encoder_length]\n",
    "            for name in self.encoder_variables\n",
    "        }\n",
    "        embeddings_varying_encoder, encoder_sparse_weights = self.encoder_variable_selection(\n",
    "            embeddings_varying_encoder,\n",
    "            static_context_variable_selection[:, :max_encoder_length],\n",
    "        )\n",
    "        embeddings_varying_decoder = {\n",
    "            name: input_vectors[name][:, max_encoder_length:]\n",
    "            for name in self.decoder_variables\n",
    "        }\n",
    "        embeddings_varying_decoder, decoder_sparse_weights = self.decoder_variable_selection(\n",
    "            embeddings_varying_decoder,\n",
    "            static_context_variable_selection[:, max_encoder_length:],\n",
    "        )\n",
    "\n",
    "        # Inicialización optimizada de estados LSTM\n",
    "        init_hidden = self.static_context_initial_hidden_lstm(static_embedding)\n",
    "        init_cell = self.static_context_initial_cell_lstm(static_embedding)\n",
    "        input_hidden = init_hidden.unsqueeze(0).expand(self.hparams.lstm_layers, -1, -1)\n",
    "        input_cell = init_cell.unsqueeze(0).expand(self.hparams.lstm_layers, -1, -1)\n",
    "\n",
    "        # Procesamiento LSTM para encoder y decoder\n",
    "        encoder_output, (hidden, cell) = self.lstm_encoder(\n",
    "            embeddings_varying_encoder,\n",
    "            (input_hidden, input_cell),\n",
    "            lengths=encoder_lengths,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "        decoder_output, _ = self.lstm_decoder(\n",
    "            embeddings_varying_decoder,\n",
    "            (hidden, cell),\n",
    "            lengths=decoder_lengths,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "\n",
    "        # Procesamiento fusionado post-LSTM para mejorar eficiencia\n",
    "        lstm_output_encoder = self.process_lstm_output(encoder_output, embeddings_varying_encoder, True)\n",
    "        lstm_output_decoder = self.process_lstm_output(decoder_output, embeddings_varying_decoder, False)\n",
    "        lstm_output = torch.cat([lstm_output_encoder, lstm_output_decoder], dim=1)\n",
    "\n",
    "        # Enriquecimiento estático\n",
    "        static_context_enrichment = self.static_context_enrichment(static_embedding)\n",
    "        attn_input = self.static_enrichment(\n",
    "            lstm_output,\n",
    "            self.expand_static_context(static_context_enrichment, timesteps),\n",
    "        )\n",
    "\n",
    "        # Usar atención eficiente en memoria para secuencias largas\n",
    "        if hasattr(self, \"efficient_attention\") and max_encoder_length > 100:\n",
    "            attn_output, attn_output_weights = self.efficient_attention(\n",
    "                q=attn_input[:, max_encoder_length:],\n",
    "                k=attn_input,\n",
    "                v=attn_input,\n",
    "                mask=self.get_attention_mask(encoder_lengths=encoder_lengths, decoder_lengths=decoder_lengths),\n",
    "            )\n",
    "        else:\n",
    "            # Atención multi-cabeza con máscara (caso estándar)\n",
    "            attn_output, attn_output_weights = self.multihead_attn(\n",
    "                q=attn_input[:, max_encoder_length:],\n",
    "                k=attn_input,\n",
    "                v=attn_input,\n",
    "                mask=self.get_attention_mask(encoder_lengths=encoder_lengths, decoder_lengths=decoder_lengths),\n",
    "            )\n",
    "\n",
    "        attn_output = self.post_attn_gate_norm(attn_output, attn_input[:, max_encoder_length:])\n",
    "        output = self.pos_wise_ff(attn_output)\n",
    "\n",
    "        # Skip connection final antes de salida\n",
    "        output = self.pre_output_gate_norm(output, lstm_output[:, max_encoder_length:])\n",
    "        \n",
    "        # Procesamiento paralelo para salida multiobjetivo\n",
    "        if self.n_targets > 1:\n",
    "            output = self.transform_multi_output(output)\n",
    "        else:\n",
    "            output = self.output_layer(output)\n",
    "\n",
    "        return self.to_network_output(\n",
    "            prediction=self.transform_output(output, target_scale=x[\"target_scale\"]),\n",
    "            encoder_attention=attn_output_weights[..., :max_encoder_length],\n",
    "            decoder_attention=attn_output_weights[..., max_encoder_length:],\n",
    "            static_variables=static_variable_selection,\n",
    "            encoder_variables=encoder_sparse_weights,\n",
    "            decoder_variables=decoder_sparse_weights,\n",
    "            decoder_lengths=decoder_lengths,\n",
    "            encoder_lengths=encoder_lengths,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c18f41-05d8-4d66-a8f6-32561f5bd45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Bloque 6: Métodos para interpretabilidad y logging\n",
    "\n",
    "    def on_fit_end(self):\n",
    "        \"\"\"\n",
    "        Se ejecuta al finalizar el entrenamiento. Si el logging está activo, se loggean las embeddings.\n",
    "        \"\"\"\n",
    "        if self.log_interval > 0:\n",
    "            self.log_embeddings()\n",
    "\n",
    "    def create_log(self, x, y, out, batch_idx, **kwargs):\n",
    "        \"\"\"\n",
    "        Crea un log de salida que incluye la interpretación de la salida si procede.\n",
    "\n",
    "        Args:\n",
    "            x: Input del modelo.\n",
    "            y: Target del modelo.\n",
    "            out: Salida del modelo.\n",
    "            batch_idx: Índice del batch.\n",
    "            **kwargs: Argumentos adicionales para logging.\n",
    "        \"\"\"\n",
    "        log = super().create_log(x, y, out, batch_idx, **kwargs)\n",
    "        if self.log_interval > 0:\n",
    "            log[\"interpretation\"] = self._log_interpretation(out)\n",
    "        return log\n",
    "\n",
    "    def _log_interpretation(self, out):\n",
    "        \"\"\"\n",
    "        Calcula la interpretación de la salida para logging.\n",
    "        \"\"\"\n",
    "        interpretation = self.interpret_output(\n",
    "            detach(out),\n",
    "            reduction=\"sum\",\n",
    "            attention_prediction_horizon=0,\n",
    "        )\n",
    "        return interpretation\n",
    "\n",
    "    def on_epoch_end(self, outputs):\n",
    "        \"\"\"\n",
    "        Se ejecuta al finalizar la época (tanto en entrenamiento como en validación) para loggear la interpretación.\n",
    "        \"\"\"\n",
    "        if self.log_interval > 0 and not self.training:\n",
    "            self.log_interpretation(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dfb742-a5b5-4a8a-9c1e-2645765c7321",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Bloque 7: Método para interpretar la salida\n",
    "\n",
    "    def interpret_output(\n",
    "        self,\n",
    "        out: Dict[str, torch.Tensor],\n",
    "        reduction: str = \"none\",\n",
    "        attention_prediction_horizon: int = 0,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Interpreta la salida del modelo.\n",
    "\n",
    "        Args:\n",
    "            out: Salida del modelo (resultado de forward).\n",
    "            reduction: Método de reducción (\"none\", \"sum\" o \"mean\").\n",
    "            attention_prediction_horizon: Horizonte de predicción para la atención.\n",
    "\n",
    "        Returns:\n",
    "            Diccionario con interpretaciones (atención, importancia de variables, etc.).\n",
    "        \"\"\"\n",
    "        batch_size = len(out[\"decoder_attention\"])\n",
    "        # Procesa atención para decoder (si es lista, combina)\n",
    "        if isinstance(out[\"decoder_attention\"], (list, tuple)):\n",
    "            max_last_dimension = max(x.size(-1) for x in out[\"decoder_attention\"])\n",
    "            first_elm = out[\"decoder_attention\"][0]\n",
    "            decoder_attention = torch.full(\n",
    "                (batch_size, *first_elm.shape[:-1], max_last_dimension),\n",
    "                float(\"nan\"),\n",
    "                dtype=first_elm.dtype,\n",
    "                device=first_elm.device,\n",
    "            )\n",
    "            for idx, att in enumerate(out[\"decoder_attention\"]):\n",
    "                decoder_length = out[\"decoder_lengths\"][idx]\n",
    "                decoder_attention[idx, :, :, :decoder_length] = att[..., :decoder_length]\n",
    "        else:\n",
    "            decoder_attention = out[\"decoder_attention\"].clone()\n",
    "            decoder_mask = create_mask(out[\"decoder_attention\"].size(1), out[\"decoder_lengths\"])\n",
    "            decoder_attention[decoder_mask[..., None, None].expand_as(decoder_attention)] = float(\"nan\")\n",
    "\n",
    "        # Procesa atención para encoder\n",
    "        if isinstance(out[\"encoder_attention\"], (list, tuple)):\n",
    "            first_elm = out[\"encoder_attention\"][0]\n",
    "            encoder_attention = torch.full(\n",
    "                (batch_size, *first_elm.shape[:-1], self.hparams.max_encoder_length),\n",
    "                float(\"nan\"),\n",
    "                dtype=first_elm.dtype,\n",
    "                device=first_elm.device,\n",
    "            )\n",
    "            for idx, att in enumerate(out[\"encoder_attention\"]):\n",
    "                encoder_length = out[\"encoder_lengths\"][idx]\n",
    "                encoder_attention[idx, :, :, self.hparams.max_encoder_length - encoder_length :] = att[..., :encoder_length]\n",
    "        else:\n",
    "            encoder_attention = out[\"encoder_attention\"].clone()\n",
    "            shifts = encoder_attention.size(3) - out[\"encoder_lengths\"]\n",
    "            new_index = (torch.arange(encoder_attention.size(3), device=encoder_attention.device)[None, None, None]\n",
    "                         .expand_as(encoder_attention) - shifts[:, None, None, None]) % encoder_attention.size(3)\n",
    "            encoder_attention = torch.gather(encoder_attention, dim=3, index=new_index)\n",
    "            if encoder_attention.size(-1) < self.hparams.max_encoder_length:\n",
    "                encoder_attention = torch.concat(\n",
    "                    [\n",
    "                        torch.full(\n",
    "                            ( *encoder_attention.shape[:-1],\n",
    "                              self.hparams.max_encoder_length - out[\"encoder_lengths\"].max(),),\n",
    "                            float(\"nan\"),\n",
    "                            dtype=encoder_attention.dtype,\n",
    "                            device=encoder_attention.device,\n",
    "                        ),\n",
    "                        encoder_attention,\n",
    "                    ],\n",
    "                    dim=-1,\n",
    "                )\n",
    "\n",
    "        attention = torch.concat([encoder_attention, decoder_attention], dim=-1)\n",
    "        attention[attention < 1e-5] = float(\"nan\")\n",
    "\n",
    "        encoder_length_histogram = integer_histogram(\n",
    "            out[\"encoder_lengths\"], min=0, max=self.hparams.max_encoder_length\n",
    "        )\n",
    "        decoder_length_histogram = integer_histogram(\n",
    "            out[\"decoder_lengths\"], min=1, max=out[\"decoder_variables\"].size(1)\n",
    "        )\n",
    "\n",
    "        # Vectorización del procesamiento de variables\n",
    "        encoder_variables = self.process_variables_importance(\n",
    "            out[\"encoder_variables\"].squeeze(-2).clone(),\n",
    "            out[\"encoder_lengths\"]\n",
    "        )\n",
    "        \n",
    "        decoder_variables = self.process_variables_importance(\n",
    "            out[\"decoder_variables\"].squeeze(-2).clone(),\n",
    "            out[\"decoder_lengths\"]\n",
    "        )\n",
    "\n",
    "        static_variables = out[\"static_variables\"].squeeze(1)\n",
    "        attention = masked_op(\n",
    "            attention[:, attention_prediction_horizon, :, : self.hparams.max_encoder_length + attention_prediction_horizon],\n",
    "            op=\"mean\", dim=1\n",
    "        )\n",
    "\n",
    "        if reduction != \"none\":\n",
    "            static_variables = static_variables.sum(dim=0)\n",
    "            encoder_variables = encoder_variables.sum(dim=0)\n",
    "            decoder_variables = decoder_variables.sum(dim=0)\n",
    "            attention = masked_op(attention, dim=0, op=reduction)\n",
    "        else:\n",
    "            attention = attention / masked_op(attention, dim=1, op=\"sum\").unsqueeze(-1)\n",
    "\n",
    "        interpretation = dict(\n",
    "            attention=attention.masked_fill(torch.isnan(attention), 0.0),\n",
    "            static_variables=static_variables,\n",
    "            encoder_variables=encoder_variables,\n",
    "            decoder_variables=decoder_variables,\n",
    "            encoder_length_histogram=encoder_length_histogram,\n",
    "            decoder_length_histogram=decoder_length_histogram,\n",
    "        )\n",
    "        return interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98475a16-f4dc-4e06-9f4f-914adff91129",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Bloque 8: Métodos para visualización\n",
    "\n",
    "    def plot_prediction(\n",
    "        self,\n",
    "        x: Dict[str, torch.Tensor],\n",
    "        out: Dict[str, torch.Tensor],\n",
    "        idx: int,\n",
    "        plot_attention: bool = True,\n",
    "        add_loss_to_title: bool = False,\n",
    "        show_future_observed: bool = True,\n",
    "        ax=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Grafica las predicciones vs. los valores reales y la atención asociada.\n",
    "\n",
    "        Args:\n",
    "            x: Input del modelo.\n",
    "            out: Salida del modelo.\n",
    "            idx: Índice de la muestra a graficar.\n",
    "            plot_attention: Si se debe graficar la atención.\n",
    "            add_loss_to_title: Si se debe agregar la pérdida en el título.\n",
    "            show_future_observed: Si se deben mostrar los valores futuros observados.\n",
    "            ax: Ejes de matplotlib para graficar.\n",
    "            **kwargs: Argumentos adicionales para plotting.\n",
    "\n",
    "        Returns:\n",
    "            Figura(s) de matplotlib con la predicción y, opcionalmente, la atención.\n",
    "        \"\"\"\n",
    "        fig = super().plot_prediction(\n",
    "            x, out, idx=idx, add_loss_to_title=add_loss_to_title, show_future_observed=show_future_observed, ax=ax, **kwargs\n",
    "        )\n",
    "        if plot_attention:\n",
    "            interpretation = self.interpret_output(out.iget(slice(idx, idx + 1)))\n",
    "            for f in to_list(fig):\n",
    "                ax = f.axes[0]\n",
    "                ax2 = ax.twinx()\n",
    "                ax2.set_ylabel(\"Attention\")\n",
    "                encoder_length = x[\"encoder_lengths\"][0]\n",
    "                ax2.plot(\n",
    "                    torch.arange(-encoder_length, 0),\n",
    "                    interpretation[\"attention\"][0, -encoder_length:].detach().cpu(),\n",
    "                    alpha=0.2,\n",
    "                    color=\"k\",\n",
    "                )\n",
    "                f.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def plot_interpretation(self, interpretation: Dict[str, torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Crea figuras que interpretan la salida del modelo: atención y pesos de variable selection.\n",
    "\n",
    "        Args:\n",
    "            interpretation: Diccionario con interpretaciones obtenidas de interpret_output.\n",
    "\n",
    "        Returns:\n",
    "            Diccionario de figuras de matplotlib.\n",
    "        \"\"\"\n",
    "        _check_matplotlib(\"plot_interpretation\")\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        figs = {}\n",
    "\n",
    "        # Gráfica de atención\n",
    "        fig, ax = plt.subplots()\n",
    "        attention = interpretation[\"attention\"].detach().cpu()\n",
    "        attention = attention / attention.sum(-1).unsqueeze(-1)\n",
    "        ax.plot(\n",
    "            np.arange(-self.hparams.max_encoder_length, attention.size(0) - self.hparams.max_encoder_length),\n",
    "            attention,\n",
    "        )\n",
    "        ax.set_xlabel(\"Time index\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.set_title(\"Attention\")\n",
    "        figs[\"attention\"] = fig\n",
    "\n",
    "        # Función auxiliar para graficar la importancia de variables\n",
    "        def make_selection_plot(title, values, labels):\n",
    "            fig, ax = plt.subplots(figsize=(7, len(values) * 0.25 + 2))\n",
    "            order = np.argsort(values)\n",
    "            values = values / values.sum(-1).unsqueeze(-1)\n",
    "            ax.barh(np.arange(len(values)), values[order] * 100, tick_label=np.asarray(labels)[order])\n",
    "            ax.set_title(title)\n",
    "            ax.set_xlabel(\"Importance in %\")\n",
    "            plt.tight_layout()\n",
    "            return fig\n",
    "\n",
    "        figs[\"static_variables\"] = make_selection_plot(\"Static variables importance\",\n",
    "                                                       interpretation[\"static_variables\"].detach().cpu(),\n",
    "                                                       self.static_variables)\n",
    "        figs[\"encoder_variables\"] = make_selection_plot(\"Encoder variables importance\",\n",
    "                                                        interpretation[\"encoder_variables\"].detach().cpu(),\n",
    "                                                        self.encoder_variables)\n",
    "        figs[\"decoder_variables\"] = make_selection_plot(\"Decoder variables importance\",\n",
    "                                                        interpretation[\"decoder_variables\"].detach().cpu(),\n",
    "                                                        self.decoder_variables)\n",
    "        return figs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab01993a-ba31-4046-97c4-1ef0342e2a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Bloque 9: Métodos para logging adicional\n",
    "\n",
    "    def log_interpretation(self, outputs):\n",
    "        \"\"\"\n",
    "        Loggea las interpretaciones en Tensorboard.\n",
    "        \"\"\"\n",
    "        interpretation = {\n",
    "            name: padded_stack(\n",
    "                [x[\"interpretation\"][name].detach() for x in outputs],\n",
    "                side=\"right\",\n",
    "                value=0,\n",
    "            ).sum(0)\n",
    "            for name in outputs[0][\"interpretation\"].keys()\n",
    "        }\n",
    "        attention_occurances = (\n",
    "            interpretation[\"encoder_length_histogram\"][1:].flip(0).float().cumsum(0)\n",
    "        )\n",
    "        attention_occurances = attention_occurances / attention_occurances.max()\n",
    "        attention_occurances = torch.cat(\n",
    "            [\n",
    "                attention_occurances,\n",
    "                torch.ones(\n",
    "                    interpretation[\"attention\"].size(0) - attention_occurances.size(0),\n",
    "                    dtype=attention_occurances.dtype,\n",
    "                    device=attention_occurances.device,\n",
    "                ),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "        interpretation[\"attention\"] = interpretation[\"attention\"] / attention_occurances.pow(2).clamp(1.0)\n",
    "        interpretation[\"attention\"] = interpretation[\"attention\"] / interpretation[\"attention\"].sum()\n",
    "\n",
    "        mpl_available = _check_matplotlib(\"log_interpretation\", raise_error=False)\n",
    "        if not mpl_available or not self._logger_supports(\"add_figure\"):\n",
    "            return None\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "        figs = self.plot_interpretation(interpretation)\n",
    "        label = self.current_stage\n",
    "        for name, fig in figs.items():\n",
    "            self.logger.experiment.add_figure(\n",
    "                f\"{label.capitalize()} {name} importance\", fig, global_step=self.global_step\n",
    "            )\n",
    "        for type in [\"encoder\", \"decoder\"]:\n",
    "            fig, ax = plt.subplots()\n",
    "            lengths = padded_stack(\n",
    "                [out[\"interpretation\"][f\"{type}_length_histogram\"] for out in outputs]\n",
    "            ).sum(0).detach().cpu()\n",
    "            start = 1 if type == \"decoder\" else 0\n",
    "            ax.plot(torch.arange(start, start + len(lengths)), lengths)\n",
    "            ax.set_xlabel(f\"{type.capitalize()} length\")\n",
    "            ax.set_ylabel(\"Number of samples\")\n",
    "            ax.set_title(f\"{type.capitalize()} length distribution in {label} epoch\")\n",
    "            self.logger.experiment.add_figure(\n",
    "                f\"{label.capitalize()} {type} length distribution\", fig, global_step=self.global_step\n",
    "            )\n",
    "\n",
    "    def log_embeddings(self):\n",
    "        \"\"\"\n",
    "        Loggea los embeddings en Tensorboard.\n",
    "        \"\"\"\n",
    "        if not self._logger_supports(\"add_embedding\"):\n",
    "            return None\n",
    "        for name, emb in self.input_embeddings.items():\n",
    "            labels = self.hparams.embedding_labels.get(name, None)\n",
    "            self.logger.experiment.add_embedding(\n",
    "                emb.weight.data.detach().cpu(),\n",
    "                metadata=labels,\n",
    "                tag=name,\n",
    "                global_step=self.global_step,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab44c44c-2978-4a4c-93eb-2802b34b7fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bloque 10: Ejemplo de uso del modelo\n",
    "# Este bloque muestra cómo se puede crear y utilizar el modelo con un dataset de ejemplo\n",
    "\n",
    "def create_example_model():\n",
    "    # Importamos las dependencias necesarias\n",
    "    from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "    from pytorch_forecasting.data.examples import generate_ar_data\n",
    "    \n",
    "    # Generamos datos de ejemplo\n",
    "    data = generate_ar_data(seasonality=10.0, timesteps=400, n_series=100)\n",
    "    \n",
    "    # Creamos el TimeSeriesDataSet\n",
    "    training_cutoff = data[\"time_idx\"].max() - 50\n",
    "    training = TimeSeriesDataSet(\n",
    "        data[data[\"time_idx\"] <= training_cutoff],\n",
    "        time_idx=\"time_idx\",\n",
    "        target=\"value\",\n",
    "        group_ids=[\"series\"],\n",
    "        max_encoder_length=30,\n",
    "        max_prediction_length=10,\n",
    "        static_categoricals=[],\n",
    "        static_reals=[],\n",
    "        time_varying_known_categoricals=[],\n",
    "        time_varying_known_reals=[\"time_idx\"],\n",
    "        time_varying_unknown_categoricals=[],\n",
    "        time_varying_unknown_reals=[\"value\"],\n",
    "    )\n",
    "    \n",
    "    # Dataset de validación\n",
    "    validation = TimeSeriesDataSet.from_dataset(training, data, min_prediction_idx=training_cutoff + 1)\n",
    "    \n",
    "    # Creamos dataloaders\n",
    "    batch_size = 128\n",
    "    train_dataloader = training.to_dataloader(train=True, batch_size=batch_size)\n",
    "    val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size)\n",
    "    \n",
    "    # Creamos el modelo\n",
    "    tft = TemporalFusionTransformer.from_dataset(\n",
    "        training,\n",
    "        learning_rate=0.03,\n",
    "        hidden_size=32,\n",
    "        attention_head_size=1,\n",
    "        dropout=0.1,\n",
    "        hidden_continuous_size=16,\n",
    "        loss=QuantileLoss(),\n",
    "        log_interval=10,\n",
    "        use_compile=True,  # usamos la optimización de compilación\n",
    "    )\n",
    "    \n",
    "    return tft, train_dataloader, val_dataloader\n",
    "\n",
    "# Ejemplo de entrenamiento (descomentar para ejecutar)\n",
    "# import pytorch_lightning as pl\n",
    "# tft, train_dataloader, val_dataloader = create_example_model()\n",
    "# trainer = pl.Trainer(max_epochs=10, accelerator='gpu', devices=1)\n",
    "# trainer.fit(tft, train_dataloader, val_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
